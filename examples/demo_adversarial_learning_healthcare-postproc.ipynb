{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness in healthcare utilization scoring model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Reference - https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_medical_expenditure.ipynb\n",
    "    </b>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial demonstrates classification model learning with bias mitigation as a part of a Care Management use case using Medical Expenditure data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook demonstrates how the AIF 360 toolkit can be used to detect and reduce bias when learning classifiers using a variety of fairness metrics and algorithms . It also demonstrates how explanations can be generated for predictions made by models learnt with the toolkit using LIME.\n",
    "\n",
    "Classifiers are built using Logistic Regression as well as Random Forests.\n",
    "\n",
    "Bias detection is demonstrated using several metrics, including disparate impact, average odds difference, statistical parity difference, equal opportunity difference, and Theil index.\n",
    "\n",
    "Bias alleviation is explored via a variety of methods, including reweighing (pre-processing algorithm), prejudice remover (in-processing algorithm), and disparate impact remover (pre-processing technique).\n",
    "\n",
    "Data from the [Medical Expenditure Panel Survey](https://meps.ahrq.gov/mepsweb/) is used in this tutorial. See [Section 2](#2.-Data-used) below for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.](#Table-of-Contents) Use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate how AIF 360 can be used to detect and mitigate bias in classfier models, we adopt the following use case:\n",
    "\n",
    "1. a data scientist develops a 'fair' healthcare utilization scoring model with respect to defined protected classes. Fairness may be dictated by legal or government regulations, such as a requirement that additional care decisions be not predicated on factors such as race of the patient.\n",
    "\n",
    "\n",
    "2. developer takes the model AND performance characteristics / specs of the model (e.g. accuracy, fairness tests, etc. basically the model factsheet) and deploys the model in an enterprise app that prioritizes cases for care management.\n",
    "\n",
    "\n",
    "3. the app is put into production and starts scoring people and making recommendations. \n",
    "\n",
    "\n",
    "4. explanations are generated for each recommendation\n",
    "\n",
    "\n",
    "5. both recommendations and associated explanations are given to nurses as a part of the care management process. The nurses can evaluate the recommendations for quality and correctness and provide feedback.\n",
    "\n",
    "\n",
    "6. nurse feedback as well as analysis of usage data with respect to specs of the model w.r.t accuracy and fairness is communicated to AI Ops specialist and LOB user periodically.\n",
    "\n",
    "\n",
    "7. when significant drift in model specs relative to the model factsheet is observed, the model is sent back for retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.](#Table-of-Contents) Data used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific data used is the [2015 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) as well as the [2016 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2015 file contains data from rounds 3,4,5 of panel 19 (2014) and rounds 1,2,3 of panel 20 (2015). The 2016 file contains data from rounds 3,4,5 of panel 20 (2015) and rounds 1,2,3 of panel 21 (2016).\n",
    "\n",
    "For this demonstration, three datasets were constructed: one from panel 19, round 5 (used for learning models), one from panel 20, round 3 (used for deployment/testing of model - steps); the other from panel 21, round 3 (used for re-training and deployment/testing of updated model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3.](#Table-of-Contents) Training models on original 2015 Panel 19 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from common_utils import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")),\n",
    "                                '..', 'data', 'raw', 'meps', 'h201.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vandanarao/PycharmProjects/AIF360_latest2611/examples/../data/raw/meps/h201.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/vandanarao/PycharmProjects/AIF360_latest2611/data/raw/meps/h201.csv' does not exist: b'/Users/vandanarao/PycharmProjects/AIF360_latest2611/data/raw/meps/h201.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4a9a734b4f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/vandanarao/PycharmProjects/AIF360_latest2611/data/raw/meps/h201.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/untitled/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/vandanarao/PycharmProjects/AIF360_latest2611/data/raw/meps/h201.csv' does not exist: b'/Users/vandanarao/PycharmProjects/AIF360_latest2611/data/raw/meps/h201.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/vandanarao/PycharmProjects/AIF360_latest2611/data/raw/meps/h201.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTSTU31X\n",
      "FTSTU42X\n",
      "FTSTU53X\n",
      "FTSTU17X\n"
     ]
    }
   ],
   "source": [
    "for s in df.columns.values:\n",
    "    if s.startswith('FTSTU'):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTSTU\n",
      "ACTDTY\n",
      "HONRDC\n",
      "RTHLTH\n",
      "MNHLTH\n",
      "CHBRON\n",
      "JTPAIN\n",
      "PREGNT\n",
      "SOCLIM\n",
      "COGLIM\n"
     ]
    }
   ],
   "source": [
    "for col in ['FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX','EDUCYR','HIDEG',\n",
    "                     'MIDX','OHRTDX','STRKDX','EMPHDX','CHBRON','CHOLDX','CANCERDX','DIABDX',\n",
    "                     'JTPAIN','ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','PREGNT','SOCLIM','COGLIM','DFHEAR42','DFSEE42','ADSMOK42',\n",
    "                     'PHQ242']:\n",
    "    if col not in cols:\n",
    "        print(col)\n",
    "#         print(col in cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mappings = {\n",
    "    'label_maps': [{1.0: '>= 10 Visits', 0.0: '< 10 Visits'}],\n",
    "    'protected_attribute_maps': [{1.0: 'White', 0.0: 'Non-White'}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_preprocessing(df):\n",
    "    \"\"\"\n",
    "    1.Create a new column, RACE that is 'White' if RACEV2X = 1 and HISPANX = 2 i.e. non Hispanic White\n",
    "      and 'Non-White' otherwise\n",
    "    2. Restrict to Panel 21\n",
    "    3. RENAME all columns that are PANEL/ROUND SPECIFIC\n",
    "    4. Drop rows based on certain values of individual features that correspond to missing/unknown - generally < -1\n",
    "    5. Compute UTILIZATION, binarize it to 0 (< 10) and 1 (>= 10)\n",
    "    \"\"\"\n",
    "    def race(row):\n",
    "        if ((row['HISPANX'] == 2) and (row['RACEV2X'] == 1)):  #non-Hispanic Whites are marked as WHITE; all others as NON-WHITE\n",
    "            return 'White'\n",
    "        return 'Non-White'\n",
    "\n",
    "    df['RACEV2X'] = df.apply(lambda row: race(row), axis=1)\n",
    "    df = df.rename(columns = {'RACEV2X' : 'RACE'})\n",
    "\n",
    "    df = df[df['PANEL'] == 21]\n",
    "\n",
    "    # RENAME COLUMNS\n",
    "    df = df.rename(columns = {'FTSTU53X' : 'FTSTU', 'ACTDTY53' : 'ACTDTY', 'HONRDC53' : 'HONRDC', 'RTHLTH53' : 'RTHLTH',\n",
    "                              'MNHLTH53' : 'MNHLTH', 'CHBRON53' : 'CHBRON', 'JTPAIN53' : 'JTPAIN', 'PREGNT53' : 'PREGNT',\n",
    "                              'WLKLIM53' : 'WLKLIM', 'ACTLIM53' : 'ACTLIM', 'SOCLIM53' : 'SOCLIM', 'COGLIM53' : 'COGLIM',\n",
    "                              'EMPST53' : 'EMPST', 'REGION53' : 'REGION', 'MARRY53X' : 'MARRY', 'AGE53X' : 'AGE',\n",
    "                              'POVCAT16' : 'POVCAT', 'INSCOV16' : 'INSCOV'})\n",
    "\n",
    "    df = df[df['REGION'] >= 0] # remove values -1\n",
    "    df = df[df['AGE'] >= 0] # remove values -1\n",
    "\n",
    "    df = df[df['MARRY'] >= 0] # remove values -1, -7, -8, -9\n",
    "\n",
    "    df = df[df['ASTHDX'] >= 0] # remove values -1, -7, -8, -9\n",
    "\n",
    "    df = df[(df[['FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX','EDUCYR','HIDEG',\n",
    "                     'MIDX','OHRTDX','STRKDX','EMPHDX','CHBRON','CHOLDX','CANCERDX','DIABDX',\n",
    "                     'JTPAIN','ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','PREGNT','SOCLIM','COGLIM','DFHEAR42','DFSEE42','ADSMOK42',\n",
    "                     'PHQ242']] >= -1).all(1)]  #for all other categorical features, remove values < -1\n",
    "\n",
    "    def utilization(row):\n",
    "        return row['OBTOTV16'] + row['OPTOTV16'] + row['ERTOT16'] + row['IPNGTD16'] + row['HHTOTD16']\n",
    "\n",
    "    df['TOTEXP16'] = df.apply(lambda row: utilization(row), axis=1)\n",
    "    lessE = df['TOTEXP16'] < 10.0\n",
    "    df.loc[lessE,'TOTEXP16'] = 0.0\n",
    "    moreE = df['TOTEXP16'] >= 10.0\n",
    "    df.loc[moreE,'TOTEXP16'] = 1.0\n",
    "\n",
    "    df = df.rename(columns = {'TOTEXP16' : 'UTILIZATION'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name='UTILIZATION'\n",
    "favorable_classes=[1.0]\n",
    "protected_attribute_names=['RACE']\n",
    "privileged_classes=[['White']]\n",
    "instance_weights_name='PERWT16F'\n",
    "categorical_features=['REGION','SEX','MARRY', 'FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX',\n",
    "                      'MIDX','OHRTDX','STRKDX','EMPHDX','CHBRON','CHOLDX','CANCERDX','DIABDX',\n",
    "                      'JTPAIN','ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','PREGNT','WLKLIM',\n",
    "                      'ACTLIM','SOCLIM','COGLIM','DFHEAR42','DFSEE42', 'ADSMOK42', 'PHQ242',\n",
    "                      'EMPST','POVCAT','INSCOV']\n",
    "features_to_keep=['REGION','AGE','SEX','RACE','MARRY',\n",
    "                     'FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX',\n",
    "                     'MIDX','OHRTDX','STRKDX','EMPHDX','CHOLDX','CANCERDX','DIABDX',\n",
    "                     'ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','DFHEAR42','DFSEE42','ADSMOK42', 'PCS42',\n",
    "                     'MCS42','K6SUM42','PHQ242','EMPST','UTILIZATION', 'PERWT16F']\n",
    "features_to_drop=[]\n",
    "na_values=[]\n",
    "custom_preprocessing=default_preprocessing\n",
    "metadata=default_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"/Users/dhanush/Documents/USC/Fall-2019/INF599/projects/AIF360/aif360/data/raw/meps/h201.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath, sep=',', na_values=na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import StandardDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['WLKLIM', 'ACTLIM', 'COGLIM', 'JTPAIN', 'POVCAT', 'CHBRON', 'INSCOV', 'SOCLIM', 'PREGNT'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c15e305e3eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfeatures_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_to_keep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mfeatures_to_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             custom_preprocessing=custom_preprocessing, metadata=metadata)\n\u001b[0m",
      "\u001b[0;32m~/Documents/USC/Fall-2019/INF599/projects/AIF360/aif360/datasets/standard_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, label_name, favorable_classes, protected_attribute_names, privileged_classes, instance_weights_name, scores_name, categorical_features, features_to_keep, features_to_drop, na_values, custom_preprocessing, metadata)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# 2. Perform dataset-specific preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcustom_preprocessing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# 3. Drop unrequested columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ca76f99df28a>\u001b[0m in \u001b[0;36mdefault_preprocessing\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     36\u001b[0m                              \u001b[0;34m'JTPAIN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ARTHDX'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ARTHTYPE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ASTHDX'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ADHDADDX'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PREGNT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WLKLIM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                              \u001b[0;34m'ACTLIM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SOCLIM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'COGLIM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DFHEAR42'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DFSEE42'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ADSMOK42'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                              'PHQ242','EMPST','POVCAT','INSCOV']] >= -1).all(1)]  #for all other categorical features, remove values < -1\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mutilization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/venv/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/venv/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/venv/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not in index\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['WLKLIM', 'ACTLIM', 'COGLIM', 'JTPAIN', 'POVCAT', 'CHBRON', 'INSCOV', 'SOCLIM', 'PREGNT'] not in index\""
     ]
    }
   ],
   "source": [
    "x = StandardDataset(df=df, label_name=label_name,\n",
    "            favorable_classes=favorable_classes,\n",
    "            protected_attribute_names=protected_attribute_names,\n",
    "            privileged_classes=privileged_classes,\n",
    "            instance_weights_name=instance_weights_name,\n",
    "            categorical_features=categorical_features,\n",
    "            features_to_keep=features_to_keep,\n",
    "            features_to_drop=features_to_drop, na_values=na_values,\n",
    "            custom_preprocessing=custom_preprocessing, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/vandanarao/.conda/envs/untitled/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "import tensorflow as tf\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing_v2 import AdversarialDebiasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load data & create splits for learning/validating/testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset and split into train (70%), test (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aif360/datasets/standard_dataset.py:121: FutureWarning: outer method for ufunc <ufunc 'equal'> is not implemented on pandas objects. Returning an ndarray, but in the future this will raise a 'NotImplementedError'. Consider explicitly converting the Series to an array with '.array' first.\n",
      "  priv = np.logical_or.reduce(np.equal.outer(vals, df[attr]))\n"
     ]
    }
   ],
   "source": [
    "(dataset_orig_panel19_train,\n",
    " dataset_orig_panel19_vt) = MEPSDataset19().split([0.6], shuffle=True)\n",
    "\n",
    "\n",
    "(dataset_orig_panel19_valid,\n",
    " dataset_orig_panel19_test) = dataset_orig_panel19_vt.split([0.5], shuffle=True)\n",
    "\n",
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                       dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                     dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used throughout the notebook to print out some labels, names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def describe(train=None, val=None, test=None):\n",
    "    if train is not None:\n",
    "        display(Markdown(\"#### Training Dataset shape\"))\n",
    "        print(train.features.shape)\n",
    "    if val is not None:\n",
    "        display(Markdown(\"#### Validation Dataset shape\"))\n",
    "        print(val.features.shape)\n",
    "    display(Markdown(\"#### Test Dataset shape\"))\n",
    "    print(test.features.shape)\n",
    "    display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "    print(test.favorable_label, test.unfavorable_label)\n",
    "    display(Markdown(\"#### Protected attribute names\"))\n",
    "    print(test.protected_attribute_names)\n",
    "    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "    print(test.privileged_protected_attributes, \n",
    "          test.unprivileged_protected_attributes)\n",
    "    display(Markdown(\"#### Dataset feature names\"))\n",
    "    print(test.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 2015 dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9498, 138)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3166, 138)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RACE']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGE', 'RACE', 'PCS42', 'MCS42', 'K6SUM42', 'REGION=1', 'REGION=2', 'REGION=3', 'REGION=4', 'SEX=1', 'SEX=2', 'MARRY=1', 'MARRY=2', 'MARRY=3', 'MARRY=4', 'MARRY=5', 'MARRY=6', 'MARRY=7', 'MARRY=8', 'MARRY=9', 'MARRY=10', 'FTSTU=-1', 'FTSTU=1', 'FTSTU=2', 'FTSTU=3', 'ACTDTY=1', 'ACTDTY=2', 'ACTDTY=3', 'ACTDTY=4', 'HONRDC=1', 'HONRDC=2', 'HONRDC=3', 'HONRDC=4', 'RTHLTH=-1', 'RTHLTH=1', 'RTHLTH=2', 'RTHLTH=3', 'RTHLTH=4', 'RTHLTH=5', 'MNHLTH=-1', 'MNHLTH=1', 'MNHLTH=2', 'MNHLTH=3', 'MNHLTH=4', 'MNHLTH=5', 'HIBPDX=-1', 'HIBPDX=1', 'HIBPDX=2', 'CHDDX=-1', 'CHDDX=1', 'CHDDX=2', 'ANGIDX=-1', 'ANGIDX=1', 'ANGIDX=2', 'MIDX=-1', 'MIDX=1', 'MIDX=2', 'OHRTDX=-1', 'OHRTDX=1', 'OHRTDX=2', 'STRKDX=-1', 'STRKDX=1', 'STRKDX=2', 'EMPHDX=-1', 'EMPHDX=1', 'EMPHDX=2', 'CHBRON=-1', 'CHBRON=1', 'CHBRON=2', 'CHOLDX=-1', 'CHOLDX=1', 'CHOLDX=2', 'CANCERDX=-1', 'CANCERDX=1', 'CANCERDX=2', 'DIABDX=-1', 'DIABDX=1', 'DIABDX=2', 'JTPAIN=-1', 'JTPAIN=1', 'JTPAIN=2', 'ARTHDX=-1', 'ARTHDX=1', 'ARTHDX=2', 'ARTHTYPE=-1', 'ARTHTYPE=1', 'ARTHTYPE=2', 'ARTHTYPE=3', 'ASTHDX=1', 'ASTHDX=2', 'ADHDADDX=-1', 'ADHDADDX=1', 'ADHDADDX=2', 'PREGNT=-1', 'PREGNT=1', 'PREGNT=2', 'WLKLIM=-1', 'WLKLIM=1', 'WLKLIM=2', 'ACTLIM=-1', 'ACTLIM=1', 'ACTLIM=2', 'SOCLIM=-1', 'SOCLIM=1', 'SOCLIM=2', 'COGLIM=-1', 'COGLIM=1', 'COGLIM=2', 'DFHEAR42=-1', 'DFHEAR42=1', 'DFHEAR42=2', 'DFSEE42=-1', 'DFSEE42=1', 'DFSEE42=2', 'ADSMOK42=-1', 'ADSMOK42=1', 'ADSMOK42=2', 'PHQ242=-1', 'PHQ242=0', 'PHQ242=1', 'PHQ242=2', 'PHQ242=3', 'PHQ242=4', 'PHQ242=5', 'PHQ242=6', 'EMPST=-1', 'EMPST=1', 'EMPST=2', 'EMPST=3', 'EMPST=4', 'POVCAT=1', 'POVCAT=2', 'POVCAT=3', 'POVCAT=4', 'POVCAT=5', 'INSCOV=1', 'INSCOV=2', 'INSCOV=3']\n"
     ]
    }
   ],
   "source": [
    "describe(dataset_orig_panel19_train, None, dataset_orig_panel19_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.49531639033990477\n"
     ]
    }
   ],
   "source": [
    "metric_orig_panel19_train = BinaryLabelDatasetMetric(\n",
    "        dataset_orig_panel19_train,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups)\n",
    "explainer_orig_panel19_train = MetricTextExplainer(metric_orig_panel19_train)\n",
    "\n",
    "print(explainer_orig_panel19_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adverserial Debiasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.133351\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.155040\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_panel19_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_panel19_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Scaled dataset - Verify that the scaling does not affect the group label statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.133351\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.155040\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_panel19_train.features = min_max_scaler.fit_transform(dataset_orig_panel19_train.features)\n",
    "dataset_orig_panel19_test.features = min_max_scaler.transform(dataset_orig_panel19_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_panel19_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_panel19_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph\n",
    "sess = tf.compat.v1.Session()\n",
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:140: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:144: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:83: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:88: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:166: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:168: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:172: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing_v2.py:193: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 0.216574; batch adversarial loss: 0.545828\n",
      "epoch 1; iter: 0; batch classifier loss: 0.655460; batch adversarial loss: 0.436933\n",
      "epoch 2; iter: 0; batch classifier loss: 0.738545; batch adversarial loss: 0.520480\n",
      "epoch 3; iter: 0; batch classifier loss: 0.584028; batch adversarial loss: 0.509030\n",
      "epoch 4; iter: 0; batch classifier loss: 0.911982; batch adversarial loss: 0.509030\n",
      "epoch 5; iter: 0; batch classifier loss: 2.191361; batch adversarial loss: 0.530691\n",
      "epoch 6; iter: 0; batch classifier loss: 2.181890; batch adversarial loss: 0.541521\n",
      "epoch 7; iter: 0; batch classifier loss: 3.023516; batch adversarial loss: 0.487369\n",
      "epoch 8; iter: 0; batch classifier loss: 4.741384; batch adversarial loss: 0.595673\n",
      "epoch 9; iter: 0; batch classifier loss: 3.696238; batch adversarial loss: 0.465708\n",
      "epoch 10; iter: 0; batch classifier loss: 5.112814; batch adversarial loss: 0.530691\n",
      "epoch 11; iter: 0; batch classifier loss: 5.839846; batch adversarial loss: 0.552352\n",
      "epoch 12; iter: 0; batch classifier loss: 6.606633; batch adversarial loss: 0.693147\n",
      "epoch 13; iter: 0; batch classifier loss: 3.778691; batch adversarial loss: 0.465708\n",
      "epoch 14; iter: 0; batch classifier loss: 6.597242; batch adversarial loss: 0.465708\n",
      "epoch 15; iter: 0; batch classifier loss: 5.483680; batch adversarial loss: 0.498200\n",
      "epoch 16; iter: 0; batch classifier loss: 5.151990; batch adversarial loss: 0.433217\n",
      "epoch 17; iter: 0; batch classifier loss: 6.393307; batch adversarial loss: 0.617334\n",
      "epoch 18; iter: 0; batch classifier loss: 7.447445; batch adversarial loss: 0.552352\n",
      "epoch 19; iter: 0; batch classifier loss: 8.649080; batch adversarial loss: 0.595673\n",
      "epoch 20; iter: 0; batch classifier loss: 8.232613; batch adversarial loss: 0.454878\n",
      "epoch 21; iter: 0; batch classifier loss: 9.959002; batch adversarial loss: 0.487369\n",
      "epoch 22; iter: 0; batch classifier loss: 13.312596; batch adversarial loss: 0.595673\n",
      "epoch 23; iter: 0; batch classifier loss: 6.891235; batch adversarial loss: 0.476539\n",
      "epoch 24; iter: 0; batch classifier loss: 9.730274; batch adversarial loss: 0.595673\n",
      "epoch 25; iter: 0; batch classifier loss: 6.373058; batch adversarial loss: 0.487369\n",
      "epoch 26; iter: 0; batch classifier loss: 14.588333; batch adversarial loss: 0.476539\n",
      "epoch 27; iter: 0; batch classifier loss: 9.949005; batch adversarial loss: 0.563182\n",
      "epoch 28; iter: 0; batch classifier loss: 13.857092; batch adversarial loss: 0.422387\n",
      "epoch 29; iter: 0; batch classifier loss: 12.064241; batch adversarial loss: 0.498200\n",
      "epoch 30; iter: 0; batch classifier loss: 9.954201; batch adversarial loss: 0.498200\n",
      "epoch 31; iter: 0; batch classifier loss: 7.905555; batch adversarial loss: 0.454878\n",
      "epoch 32; iter: 0; batch classifier loss: 5.361988; batch adversarial loss: 0.422387\n",
      "epoch 33; iter: 0; batch classifier loss: 12.669464; batch adversarial loss: 0.574013\n",
      "epoch 34; iter: 0; batch classifier loss: 9.132454; batch adversarial loss: 0.509030\n",
      "epoch 35; iter: 0; batch classifier loss: 10.736711; batch adversarial loss: 0.574013\n",
      "epoch 36; iter: 0; batch classifier loss: 16.673592; batch adversarial loss: 0.509030\n",
      "epoch 37; iter: 0; batch classifier loss: 12.935251; batch adversarial loss: 0.465708\n",
      "epoch 38; iter: 0; batch classifier loss: 11.626402; batch adversarial loss: 0.400726\n",
      "epoch 39; iter: 0; batch classifier loss: 8.130829; batch adversarial loss: 0.552352\n",
      "epoch 40; iter: 0; batch classifier loss: 10.581896; batch adversarial loss: 0.487369\n",
      "epoch 41; iter: 0; batch classifier loss: 14.085188; batch adversarial loss: 0.454878\n",
      "epoch 42; iter: 0; batch classifier loss: 15.331909; batch adversarial loss: 0.444047\n",
      "epoch 43; iter: 0; batch classifier loss: 9.344410; batch adversarial loss: 0.444047\n",
      "epoch 44; iter: 0; batch classifier loss: 13.446939; batch adversarial loss: 0.444047\n",
      "epoch 45; iter: 0; batch classifier loss: 10.758040; batch adversarial loss: 0.454878\n",
      "epoch 46; iter: 0; batch classifier loss: 12.043358; batch adversarial loss: 0.519860\n",
      "epoch 47; iter: 0; batch classifier loss: 11.375336; batch adversarial loss: 0.476539\n",
      "epoch 48; iter: 0; batch classifier loss: 14.651360; batch adversarial loss: 0.411556\n",
      "epoch 49; iter: 0; batch classifier loss: 13.229342; batch adversarial loss: 0.444047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing_v2.AdversarialDebiasing at 0x1a40652d90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.fit(dataset_orig_panel19_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'debiased_classifier/classifier_model/Softmax:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ind = np.where(np.asarray([0., 1.])== dataset_orig_panel19_train.favorable_label)[0][0]\n",
    "pos_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "#dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "#dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
    "#dataset_valid_pred = plain_model.predict(dataset_orig_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_panel19_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_debiasing_valid = debiased_model.predict(dataset_orig_panel19_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.87913655e-08],\n",
       "       [1.00000000e+00],\n",
       "       [7.48751132e-16],\n",
       "       ...,\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_debiasing_valid.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0011514 ],\n",
       "       [0.00255091],\n",
       "       [0.19314738],\n",
       "       ...,\n",
       "       [0.00053161],\n",
       "       [0.00595715],\n",
       "       [0.10970578]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_debiasing_test.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999988],\n",
       "       [0.        ],\n",
       "       [1.        ],\n",
       "       ...,\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_debiasing_valid.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99884856],\n",
       "       [0.9974491 ],\n",
       "       [0.80685264],\n",
       "       ...,\n",
       "       [0.99946839],\n",
       "       [0.99404281],\n",
       "       [0.89029425]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_debiasing_test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.87913655e-08],\n",
       "       [1.00000000e+00],\n",
       "       [7.48751132e-16],\n",
       "       ...,\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00],\n",
       "       [1.00000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_panel19_train.features)\n",
    "y_train = dataset_orig_panel19_train.labels.ravel()\n",
    "\n",
    "\n",
    "dataset_orig_panel19_valid_pred = dataset_orig_panel19_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_panel19_valid_pred.features)\n",
    "y_valid = dataset_orig_panel19_valid_pred.labels\n",
    "#dataset_orig_test_pred.scores = dataset_nodebiasing_test.labels.reshape(-1,1)\n",
    "#dataset_orig_valid_pred.scores = plain_model.predict((X_valid)[:,pos_ind].reshape(-1,1))\n",
    "dataset_orig_panel19_valid_pred.scores = dataset_debiasing_valid.probs.reshape(-1,1)\n",
    "dataset_orig_panel19_valid_pred.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0011514 ],\n",
       "       [0.00255091],\n",
       "       [0.19314738],\n",
       "       ...,\n",
       "       [0.00053161],\n",
       "       [0.00595715],\n",
       "       [0.10970578]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_panel19_train.features)\n",
    "y_train = dataset_orig_panel19_train.labels.ravel()\n",
    "\n",
    "\n",
    "dataset_orig_panel19_test_pred = dataset_orig_panel19_test.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_panel19_test_pred.features)\n",
    "y_valid = dataset_orig_panel19_test_pred.labels\n",
    "#dataset_orig_test_pred.scores = dataset_nodebiasing_test.labels.reshape(-1,1)\n",
    "#dataset_orig_valid_pred.scores = plain_model.predict((X_valid)[:,pos_ind].reshape(-1,1))\n",
    "dataset_orig_panel19_test_pred.scores = dataset_debiasing_test.probs.reshape(-1,1)\n",
    "dataset_orig_panel19_test_pred.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n",
    "#random seed for calibrated equal odds prediction\n",
    "randseed = 12345679 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds equalizing post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# Learn parameters to equalize odds and apply to create a new dataset\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=randseed)\n",
    "cpp = cpp.fit(dataset_orig_panel19_valid, dataset_orig_panel19_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds equalizing post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# Learn parameters to equalize odds and apply to create a new dataset\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=randseed)\n",
    "cpp = cpp.fit(dataset_orig_panel19_test, dataset_orig_panel19_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_transf_valid_pred = cpp.predict(dataset_orig_panel19_valid_pred)\n",
    "dataset_transf_test_pred = cpp.predict(dataset_orig_panel19_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed validation dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in GFPR between unprivileged and privileged groups\n",
      "-0.05674665158367431\n",
      "Difference in GFNR between unprivileged and privileged groups\n",
      "-0.00035573091750576435\n",
      "Test set: Classification accuracy = 0.497713\n"
     ]
    }
   ],
   "source": [
    "cm_transf_valid = ClassificationMetric(dataset_orig_panel19_valid, dataset_transf_valid_pred,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original-Transformed validation dataset\"))\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_transf_valid.difference(cm_transf_valid.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_transf_valid.difference(cm_transf_valid.generalized_false_negative_rate))\n",
    "print(\"Test set: Classification accuracy = %f\" % cm_transf_valid.accuracy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.995532\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original-Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in GFPR between unprivileged and privileged groups\n",
      "0.016652374763453805\n",
      "Difference in GFNR between unprivileged and privileged groups\n",
      "0.07274757736886617\n"
     ]
    }
   ],
   "source": [
    "cm_transf_test = ClassificationMetric(dataset_orig_panel19_test, dataset_transf_test_pred,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % cm_transf_test.accuracy())\n",
    "display(Markdown(\"#### Original-Transformed testing dataset\"))\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_transf_test.difference(cm_transf_test.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_transf_test.difference(cm_transf_test.generalized_false_negative_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6032\n",
      "Statistical parity difference = -0.0355\n",
      "Disparate impact = 0.9445\n",
      "Average odds difference = 0.0060\n",
      "Equal opportunity difference = 0.0325\n",
      "Theil index = 0.0971\n"
     ]
    }
   ],
   "source": [
    "metric_valid_bef = compute_metrics(dataset_orig_panel19_valid, dataset_transf_valid_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.9903\n",
      "Statistical parity difference = -0.1665\n",
      "Disparate impact = 0.4280\n",
      "Average odds difference = -0.0421\n",
      "Equal opportunity difference = -0.0842\n",
      "Theil index = 0.0079\n"
     ]
    }
   ],
   "source": [
    "metric_valid_bef = compute_metrics(dataset_orig_panel19_test, dataset_transf_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
