{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of adversarial debiasing algorithm to learn a fair classifier.\n",
    "Adversarial debiasing [1] is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit. We will see how to use this algorithm for learning models with and without fairness constraints and apply them on the Adult dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "\n",
    "#from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing_v2 import AdversarialDebiasing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "\n",
    "from common_utils import compute_metrics\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_adult()\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "#dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34189, 18)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.191848\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.199053\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Scaled dataset - Verify that the scaling does not affect the group label statistics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.191848\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.199053\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn plan classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:140: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:144: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:84: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:89: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:168: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:170: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:174: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From ../aif360/algorithms/inprocessing/adversarial_debiasing.py:195: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 0.402223; batch adversarial loss: 0.923451\n",
      "epoch 0; iter: 200; batch classifier loss: 0.562122; batch adversarial loss: 0.955835\n",
      "epoch 1; iter: 0; batch classifier loss: 0.795309; batch adversarial loss: 0.941956\n",
      "epoch 1; iter: 200; batch classifier loss: 0.478142; batch adversarial loss: 0.898928\n",
      "epoch 2; iter: 0; batch classifier loss: 0.630311; batch adversarial loss: 0.953077\n",
      "epoch 2; iter: 200; batch classifier loss: 0.643377; batch adversarial loss: 0.888095\n",
      "epoch 3; iter: 0; batch classifier loss: 0.680284; batch adversarial loss: 0.931417\n",
      "epoch 3; iter: 200; batch classifier loss: 0.760778; batch adversarial loss: 0.953077\n",
      "epoch 4; iter: 0; batch classifier loss: 0.671574; batch adversarial loss: 0.996399\n",
      "epoch 4; iter: 200; batch classifier loss: 1.354705; batch adversarial loss: 0.909756\n",
      "epoch 5; iter: 0; batch classifier loss: 0.834416; batch adversarial loss: 0.812282\n",
      "epoch 5; iter: 200; batch classifier loss: 1.276924; batch adversarial loss: 0.877264\n",
      "epoch 6; iter: 0; batch classifier loss: 1.883076; batch adversarial loss: 0.942247\n",
      "epoch 6; iter: 200; batch classifier loss: 2.200733; batch adversarial loss: 0.909756\n",
      "epoch 7; iter: 0; batch classifier loss: 1.480465; batch adversarial loss: 0.931417\n",
      "epoch 7; iter: 200; batch classifier loss: 1.541778; batch adversarial loss: 1.018060\n",
      "epoch 8; iter: 0; batch classifier loss: 1.387632; batch adversarial loss: 0.963908\n",
      "epoch 8; iter: 200; batch classifier loss: 1.963269; batch adversarial loss: 0.866434\n",
      "epoch 9; iter: 0; batch classifier loss: 2.166685; batch adversarial loss: 0.888095\n",
      "epoch 9; iter: 200; batch classifier loss: 2.701533; batch adversarial loss: 1.028890\n",
      "epoch 10; iter: 0; batch classifier loss: 2.366400; batch adversarial loss: 0.866434\n",
      "epoch 10; iter: 200; batch classifier loss: 1.957845; batch adversarial loss: 0.985569\n",
      "epoch 11; iter: 0; batch classifier loss: 2.521912; batch adversarial loss: 0.877264\n",
      "epoch 11; iter: 200; batch classifier loss: 2.475095; batch adversarial loss: 0.974738\n",
      "epoch 12; iter: 0; batch classifier loss: 2.385114; batch adversarial loss: 0.888095\n",
      "epoch 12; iter: 200; batch classifier loss: 1.897044; batch adversarial loss: 0.920586\n",
      "epoch 13; iter: 0; batch classifier loss: 1.999140; batch adversarial loss: 0.898925\n",
      "epoch 13; iter: 200; batch classifier loss: 2.971869; batch adversarial loss: 0.942247\n",
      "epoch 14; iter: 0; batch classifier loss: 1.339699; batch adversarial loss: 0.996399\n",
      "epoch 14; iter: 200; batch classifier loss: 2.310631; batch adversarial loss: 0.898925\n",
      "epoch 15; iter: 0; batch classifier loss: 2.516250; batch adversarial loss: 0.898925\n",
      "epoch 15; iter: 200; batch classifier loss: 3.328509; batch adversarial loss: 0.909756\n",
      "epoch 16; iter: 0; batch classifier loss: 3.274787; batch adversarial loss: 1.104703\n",
      "epoch 16; iter: 200; batch classifier loss: 3.348932; batch adversarial loss: 0.974738\n",
      "epoch 17; iter: 0; batch classifier loss: 1.189365; batch adversarial loss: 0.877264\n",
      "epoch 17; iter: 200; batch classifier loss: 3.659649; batch adversarial loss: 0.920586\n",
      "epoch 18; iter: 0; batch classifier loss: 2.622293; batch adversarial loss: 1.018060\n",
      "epoch 18; iter: 200; batch classifier loss: 4.279868; batch adversarial loss: 0.898925\n",
      "epoch 19; iter: 0; batch classifier loss: 3.821998; batch adversarial loss: 0.898925\n",
      "epoch 19; iter: 200; batch classifier loss: 2.224851; batch adversarial loss: 0.963908\n",
      "epoch 20; iter: 0; batch classifier loss: 3.104690; batch adversarial loss: 0.909756\n",
      "epoch 20; iter: 200; batch classifier loss: 2.282924; batch adversarial loss: 0.866434\n",
      "epoch 21; iter: 0; batch classifier loss: 2.528828; batch adversarial loss: 0.877264\n",
      "epoch 21; iter: 200; batch classifier loss: 2.120339; batch adversarial loss: 0.920586\n",
      "epoch 22; iter: 0; batch classifier loss: 2.220504; batch adversarial loss: 0.855604\n",
      "epoch 22; iter: 200; batch classifier loss: 3.656760; batch adversarial loss: 0.931417\n",
      "epoch 23; iter: 0; batch classifier loss: 2.642974; batch adversarial loss: 0.963908\n",
      "epoch 23; iter: 200; batch classifier loss: 3.304755; batch adversarial loss: 0.866434\n",
      "epoch 24; iter: 0; batch classifier loss: 3.070771; batch adversarial loss: 0.909756\n",
      "epoch 24; iter: 200; batch classifier loss: 3.101422; batch adversarial loss: 0.877264\n",
      "epoch 25; iter: 0; batch classifier loss: 3.046875; batch adversarial loss: 0.898925\n",
      "epoch 25; iter: 200; batch classifier loss: 5.012580; batch adversarial loss: 1.018060\n",
      "epoch 26; iter: 0; batch classifier loss: 3.094613; batch adversarial loss: 0.877264\n",
      "epoch 26; iter: 200; batch classifier loss: 3.970491; batch adversarial loss: 0.833943\n",
      "epoch 27; iter: 0; batch classifier loss: 4.570089; batch adversarial loss: 0.942247\n",
      "epoch 27; iter: 200; batch classifier loss: 3.053542; batch adversarial loss: 0.898925\n",
      "epoch 28; iter: 0; batch classifier loss: 3.369502; batch adversarial loss: 0.931417\n",
      "epoch 28; iter: 200; batch classifier loss: 1.398960; batch adversarial loss: 0.812282\n",
      "epoch 29; iter: 0; batch classifier loss: 4.407803; batch adversarial loss: 1.039721\n",
      "epoch 29; iter: 200; batch classifier loss: 2.134377; batch adversarial loss: 0.888095\n",
      "epoch 30; iter: 0; batch classifier loss: 2.683743; batch adversarial loss: 0.909756\n",
      "epoch 30; iter: 200; batch classifier loss: 2.802760; batch adversarial loss: 0.877264\n",
      "epoch 31; iter: 0; batch classifier loss: 2.276317; batch adversarial loss: 0.833943\n",
      "epoch 31; iter: 200; batch classifier loss: 3.744196; batch adversarial loss: 0.898925\n",
      "epoch 32; iter: 0; batch classifier loss: 3.561285; batch adversarial loss: 0.953077\n",
      "epoch 32; iter: 200; batch classifier loss: 4.840140; batch adversarial loss: 0.974738\n",
      "epoch 33; iter: 0; batch classifier loss: 3.999545; batch adversarial loss: 0.942247\n",
      "epoch 33; iter: 200; batch classifier loss: 3.474352; batch adversarial loss: 0.823112\n",
      "epoch 34; iter: 0; batch classifier loss: 2.471766; batch adversarial loss: 0.898925\n",
      "epoch 34; iter: 200; batch classifier loss: 3.980943; batch adversarial loss: 1.007230\n",
      "epoch 35; iter: 0; batch classifier loss: 4.254687; batch adversarial loss: 0.844773\n",
      "epoch 35; iter: 200; batch classifier loss: 4.418768; batch adversarial loss: 0.866434\n",
      "epoch 36; iter: 0; batch classifier loss: 2.762636; batch adversarial loss: 0.942247\n",
      "epoch 36; iter: 200; batch classifier loss: 3.126775; batch adversarial loss: 0.931417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37; iter: 0; batch classifier loss: 3.549010; batch adversarial loss: 0.833943\n",
      "epoch 37; iter: 200; batch classifier loss: 4.056004; batch adversarial loss: 0.996399\n",
      "epoch 38; iter: 0; batch classifier loss: 2.787884; batch adversarial loss: 0.877264\n",
      "epoch 38; iter: 200; batch classifier loss: 2.900218; batch adversarial loss: 0.855604\n",
      "epoch 39; iter: 0; batch classifier loss: 3.637752; batch adversarial loss: 0.974738\n",
      "epoch 39; iter: 200; batch classifier loss: 3.876347; batch adversarial loss: 0.877264\n",
      "epoch 40; iter: 0; batch classifier loss: 4.137067; batch adversarial loss: 0.909756\n",
      "epoch 40; iter: 200; batch classifier loss: 3.669425; batch adversarial loss: 0.888095\n",
      "epoch 41; iter: 0; batch classifier loss: 4.584616; batch adversarial loss: 0.920586\n",
      "epoch 41; iter: 200; batch classifier loss: 3.576030; batch adversarial loss: 0.844773\n",
      "epoch 42; iter: 0; batch classifier loss: 4.525702; batch adversarial loss: 0.888095\n",
      "epoch 42; iter: 200; batch classifier loss: 3.447188; batch adversarial loss: 0.963908\n",
      "epoch 43; iter: 0; batch classifier loss: 3.294439; batch adversarial loss: 0.996399\n",
      "epoch 43; iter: 200; batch classifier loss: 3.401023; batch adversarial loss: 0.920586\n",
      "epoch 44; iter: 0; batch classifier loss: 3.756185; batch adversarial loss: 0.909756\n",
      "epoch 44; iter: 200; batch classifier loss: 3.905788; batch adversarial loss: 0.909756\n",
      "epoch 45; iter: 0; batch classifier loss: 4.860614; batch adversarial loss: 0.866434\n",
      "epoch 45; iter: 200; batch classifier loss: 3.030940; batch adversarial loss: 0.996399\n",
      "epoch 46; iter: 0; batch classifier loss: 3.690849; batch adversarial loss: 0.985569\n",
      "epoch 46; iter: 200; batch classifier loss: 5.131287; batch adversarial loss: 0.974738\n",
      "epoch 47; iter: 0; batch classifier loss: 3.975397; batch adversarial loss: 0.974738\n",
      "epoch 47; iter: 200; batch classifier loss: 2.868990; batch adversarial loss: 0.877264\n",
      "epoch 48; iter: 0; batch classifier loss: 2.841128; batch adversarial loss: 0.985569\n",
      "epoch 48; iter: 200; batch classifier loss: 3.605910; batch adversarial loss: 0.985569\n",
      "epoch 49; iter: 0; batch classifier loss: 2.785342; batch adversarial loss: 0.866434\n",
      "epoch 49; iter: 200; batch classifier loss: 4.516659; batch adversarial loss: 0.844773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x13ccd6860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'plain_classifier/classifier_model/Softmax:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model.pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AdversarialDebiasing' object has no attribute 'y_pred_cls'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7d08d8667f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'AdversarialDebiasing' object has no attribute 'y_pred_cls'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "plain_model.y_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_ind = np.where(np.asarray([0., 1.])== dataset_orig_train.favorable_label)[0][0]\n",
    "pos_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "#dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
    "#dataset_valid_pred = plain_model.predict(dataset_orig_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8976953 ]\n",
      " [0.82502651]\n",
      " [0.82502651]\n",
      " ...\n",
      " [0.77253067]\n",
      " [0.81346083]\n",
      " [0.78133065]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_nodebiasing_test.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10230471]\n",
      " [0.17497346]\n",
      " [0.17497346]\n",
      " ...\n",
      " [0.22746934]\n",
      " [0.18653922]\n",
      " [0.21866938]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_nodebiasing_test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8976953 ],\n",
       "       [0.82502651],\n",
       "       [0.82502651],\n",
       "       ...,\n",
       "       [0.77253067],\n",
       "       [0.81346083],\n",
       "       [0.78133065]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_valid = dataset_orig_test_pred.labels\n",
    "#dataset_orig_test_pred.scores = dataset_nodebiasing_test.labels.reshape(-1,1)\n",
    "#dataset_orig_valid_pred.scores = plain_model.predict((X_valid)[:,pos_ind].reshape(-1,1))\n",
    "dataset_orig_test_pred.scores = dataset_nodebiasing_test.probs.reshape(-1,1)\n",
    "dataset_orig_test_pred.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85585117],\n",
       "       [0.83548355],\n",
       "       [0.85293543],\n",
       "       ...,\n",
       "       [0.89478946],\n",
       "       [0.85585117],\n",
       "       [0.715424  ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "\n",
    "dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_valid_pred.features)\n",
    "y_valid = dataset_orig_valid_pred.labels\n",
    "#dataset_orig_test_pred.scores = dataset_nodebiasing_test.labels.reshape(-1,1)\n",
    "#dataset_orig_valid_pred.scores = plain_model.predict((X_valid)[:,pos_ind].reshape(-1,1))\n",
    "dataset_orig_valid_pred.scores = dataset_valid_pred.probs.reshape(-1,1)\n",
    "dataset_orig_valid_pred.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.6759\n",
      "Optimal classification threshold (no fairness constraints) = 0.8316\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_test_pred.scores > class_thresh\n",
    "    dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "    dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_test,\n",
    "                                             dataset_orig_test_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=\"Statistical parity difference\",\n",
    "                                  metric_ub=0.001, metric_lb=-0.001)\n",
    "ROC = ROC.fit(dataset_orig_test, dataset_orig_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.7920\n",
      "Optimal ROC margin = 0.0340\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6759\n",
      "Statistical parity difference = -0.4856\n",
      "Disparate impact = 0.1348\n",
      "Average odds difference = -0.4423\n",
      "Equal opportunity difference = -0.4512\n",
      "Theil index = 0.1397\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_valid_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5727\n",
      "Statistical parity difference = 0.0003\n",
      "Disparate impact = 1.0005\n",
      "Average odds difference = 0.0363\n",
      "Equal opportunity difference = 0.0442\n",
      "Theil index = 0.1161\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(metric_valid_aft[\"Statistical parity difference\"]) <= np.abs(metric_valid_bef[\"Statistical parity difference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.6835\n",
      "Optimal classification threshold (no fairness constraints) = 0.8316\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_orig_valid_pred.scores > class_thresh\n",
    "    dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "    dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_valid,\n",
    "                                             dataset_orig_valid_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=\"Statistical parity difference\",\n",
    "                                  metric_ub=0.001, metric_lb=-0.001)\n",
    "ROC = ROC.fit(dataset_orig_valid, dataset_orig_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.5940\n",
      "Optimal ROC margin = 0.0414\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6835\n",
      "Statistical parity difference = -0.4668\n",
      "Disparate impact = 0.1573\n",
      "Average odds difference = -0.4324\n",
      "Equal opportunity difference = -0.4595\n",
      "Theil index = 0.1351\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_valid_pred.scores > best_class_thresh\n",
    "dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_valid_bef = compute_metrics(dataset_orig_valid, dataset_orig_valid_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5017\n",
      "Statistical parity difference = 0.0005\n",
      "Disparate impact = 1.0005\n",
      "Average odds difference = 0.0013\n",
      "Equal opportunity difference = 0.0013\n",
      "Theil index = 0.0341\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_valid_pred = ROC.predict(dataset_orig_valid_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred, \n",
    "                unprivileged_groups, privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(metric_valid_aft[\"Statistical parity difference\"]) <= np.abs(metric_valid_bef[\"Statistical parity difference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm based on adversarial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.707958; batch adversarial loss: 0.673177\n",
      "epoch 0; iter: 200; batch classifier loss: 0.386025; batch adversarial loss: 0.627082\n",
      "epoch 1; iter: 0; batch classifier loss: 0.379295; batch adversarial loss: 0.651693\n",
      "epoch 1; iter: 200; batch classifier loss: 0.480181; batch adversarial loss: 0.614757\n",
      "epoch 2; iter: 0; batch classifier loss: 0.448960; batch adversarial loss: 0.632887\n",
      "epoch 2; iter: 200; batch classifier loss: 0.437593; batch adversarial loss: 0.612206\n",
      "epoch 3; iter: 0; batch classifier loss: 0.404891; batch adversarial loss: 0.639086\n",
      "epoch 3; iter: 200; batch classifier loss: 0.479020; batch adversarial loss: 0.642401\n",
      "epoch 4; iter: 0; batch classifier loss: 0.383114; batch adversarial loss: 0.583473\n",
      "epoch 4; iter: 200; batch classifier loss: 0.481365; batch adversarial loss: 0.651561\n",
      "epoch 5; iter: 0; batch classifier loss: 0.374620; batch adversarial loss: 0.603537\n",
      "epoch 5; iter: 200; batch classifier loss: 0.412906; batch adversarial loss: 0.647640\n",
      "epoch 6; iter: 0; batch classifier loss: 0.449114; batch adversarial loss: 0.608910\n",
      "epoch 6; iter: 200; batch classifier loss: 0.506916; batch adversarial loss: 0.566510\n",
      "epoch 7; iter: 0; batch classifier loss: 0.428516; batch adversarial loss: 0.655776\n",
      "epoch 7; iter: 200; batch classifier loss: 0.478135; batch adversarial loss: 0.550312\n",
      "epoch 8; iter: 0; batch classifier loss: 0.478586; batch adversarial loss: 0.624218\n",
      "epoch 8; iter: 200; batch classifier loss: 0.461630; batch adversarial loss: 0.587733\n",
      "epoch 9; iter: 0; batch classifier loss: 0.465517; batch adversarial loss: 0.656298\n",
      "epoch 9; iter: 200; batch classifier loss: 0.388366; batch adversarial loss: 0.601929\n",
      "epoch 10; iter: 0; batch classifier loss: 0.471613; batch adversarial loss: 0.609384\n",
      "epoch 10; iter: 200; batch classifier loss: 0.357756; batch adversarial loss: 0.580793\n",
      "epoch 11; iter: 0; batch classifier loss: 0.426220; batch adversarial loss: 0.583863\n",
      "epoch 11; iter: 200; batch classifier loss: 0.388512; batch adversarial loss: 0.634092\n",
      "epoch 12; iter: 0; batch classifier loss: 0.442726; batch adversarial loss: 0.630233\n",
      "epoch 12; iter: 200; batch classifier loss: 0.398993; batch adversarial loss: 0.610744\n",
      "epoch 13; iter: 0; batch classifier loss: 0.432027; batch adversarial loss: 0.571084\n",
      "epoch 13; iter: 200; batch classifier loss: 0.495332; batch adversarial loss: 0.625368\n",
      "epoch 14; iter: 0; batch classifier loss: 0.424025; batch adversarial loss: 0.618043\n",
      "epoch 14; iter: 200; batch classifier loss: 0.465098; batch adversarial loss: 0.574209\n",
      "epoch 15; iter: 0; batch classifier loss: 0.434493; batch adversarial loss: 0.634880\n",
      "epoch 15; iter: 200; batch classifier loss: 0.341261; batch adversarial loss: 0.679203\n",
      "epoch 16; iter: 0; batch classifier loss: 0.422426; batch adversarial loss: 0.588591\n",
      "epoch 16; iter: 200; batch classifier loss: 0.416332; batch adversarial loss: 0.642615\n",
      "epoch 17; iter: 0; batch classifier loss: 0.582333; batch adversarial loss: 0.581737\n",
      "epoch 17; iter: 200; batch classifier loss: 0.394016; batch adversarial loss: 0.602605\n",
      "epoch 18; iter: 0; batch classifier loss: 0.373767; batch adversarial loss: 0.672464\n",
      "epoch 18; iter: 200; batch classifier loss: 0.403650; batch adversarial loss: 0.601442\n",
      "epoch 19; iter: 0; batch classifier loss: 0.427753; batch adversarial loss: 0.628340\n",
      "epoch 19; iter: 200; batch classifier loss: 0.436492; batch adversarial loss: 0.631843\n",
      "epoch 20; iter: 0; batch classifier loss: 0.371804; batch adversarial loss: 0.675799\n",
      "epoch 20; iter: 200; batch classifier loss: 0.436083; batch adversarial loss: 0.625920\n",
      "epoch 21; iter: 0; batch classifier loss: 0.483453; batch adversarial loss: 0.615056\n",
      "epoch 21; iter: 200; batch classifier loss: 0.419765; batch adversarial loss: 0.596082\n",
      "epoch 22; iter: 0; batch classifier loss: 0.358551; batch adversarial loss: 0.610531\n",
      "epoch 22; iter: 200; batch classifier loss: 0.409824; batch adversarial loss: 0.650517\n",
      "epoch 23; iter: 0; batch classifier loss: 0.373932; batch adversarial loss: 0.620941\n",
      "epoch 23; iter: 200; batch classifier loss: 0.390170; batch adversarial loss: 0.641507\n",
      "epoch 24; iter: 0; batch classifier loss: 0.468513; batch adversarial loss: 0.654972\n",
      "epoch 24; iter: 200; batch classifier loss: 0.423033; batch adversarial loss: 0.646138\n",
      "epoch 25; iter: 0; batch classifier loss: 0.396675; batch adversarial loss: 0.672927\n",
      "epoch 25; iter: 200; batch classifier loss: 0.329552; batch adversarial loss: 0.667641\n",
      "epoch 26; iter: 0; batch classifier loss: 0.414097; batch adversarial loss: 0.563713\n",
      "epoch 26; iter: 200; batch classifier loss: 0.473469; batch adversarial loss: 0.565462\n",
      "epoch 27; iter: 0; batch classifier loss: 0.552828; batch adversarial loss: 0.655219\n",
      "epoch 27; iter: 200; batch classifier loss: 0.517633; batch adversarial loss: 0.549480\n",
      "epoch 28; iter: 0; batch classifier loss: 0.452951; batch adversarial loss: 0.578402\n",
      "epoch 28; iter: 200; batch classifier loss: 0.412827; batch adversarial loss: 0.618669\n",
      "epoch 29; iter: 0; batch classifier loss: 0.400665; batch adversarial loss: 0.509105\n",
      "epoch 29; iter: 200; batch classifier loss: 0.381553; batch adversarial loss: 0.595373\n",
      "epoch 30; iter: 0; batch classifier loss: 0.337817; batch adversarial loss: 0.639863\n",
      "epoch 30; iter: 200; batch classifier loss: 0.436163; batch adversarial loss: 0.622022\n",
      "epoch 31; iter: 0; batch classifier loss: 0.375991; batch adversarial loss: 0.536775\n",
      "epoch 31; iter: 200; batch classifier loss: 0.437749; batch adversarial loss: 0.580260\n",
      "epoch 32; iter: 0; batch classifier loss: 0.393956; batch adversarial loss: 0.585647\n",
      "epoch 32; iter: 200; batch classifier loss: 0.452549; batch adversarial loss: 0.637326\n",
      "epoch 33; iter: 0; batch classifier loss: 0.440199; batch adversarial loss: 0.593393\n",
      "epoch 33; iter: 200; batch classifier loss: 0.459921; batch adversarial loss: 0.570710\n",
      "epoch 34; iter: 0; batch classifier loss: 0.445385; batch adversarial loss: 0.607842\n",
      "epoch 34; iter: 200; batch classifier loss: 0.430971; batch adversarial loss: 0.638968\n",
      "epoch 35; iter: 0; batch classifier loss: 0.359516; batch adversarial loss: 0.618149\n",
      "epoch 35; iter: 200; batch classifier loss: 0.513692; batch adversarial loss: 0.622428\n",
      "epoch 36; iter: 0; batch classifier loss: 0.386845; batch adversarial loss: 0.583297\n",
      "epoch 36; iter: 200; batch classifier loss: 0.494921; batch adversarial loss: 0.586599\n",
      "epoch 37; iter: 0; batch classifier loss: 0.431735; batch adversarial loss: 0.604705\n",
      "epoch 37; iter: 200; batch classifier loss: 0.447507; batch adversarial loss: 0.618695\n",
      "epoch 38; iter: 0; batch classifier loss: 0.459307; batch adversarial loss: 0.633271\n",
      "epoch 38; iter: 200; batch classifier loss: 0.344688; batch adversarial loss: 0.708827\n",
      "epoch 39; iter: 0; batch classifier loss: 0.481141; batch adversarial loss: 0.652617\n",
      "epoch 39; iter: 200; batch classifier loss: 0.365180; batch adversarial loss: 0.608188\n",
      "epoch 40; iter: 0; batch classifier loss: 0.447466; batch adversarial loss: 0.598436\n",
      "epoch 40; iter: 200; batch classifier loss: 0.446186; batch adversarial loss: 0.564406\n",
      "epoch 41; iter: 0; batch classifier loss: 0.389541; batch adversarial loss: 0.542489\n",
      "epoch 41; iter: 200; batch classifier loss: 0.493515; batch adversarial loss: 0.542254\n",
      "epoch 42; iter: 0; batch classifier loss: 0.395086; batch adversarial loss: 0.638168\n",
      "epoch 42; iter: 200; batch classifier loss: 0.373830; batch adversarial loss: 0.540452\n",
      "epoch 43; iter: 0; batch classifier loss: 0.446512; batch adversarial loss: 0.612458\n",
      "epoch 43; iter: 200; batch classifier loss: 0.495085; batch adversarial loss: 0.590592\n",
      "epoch 44; iter: 0; batch classifier loss: 0.441519; batch adversarial loss: 0.620180\n",
      "epoch 44; iter: 200; batch classifier loss: 0.367827; batch adversarial loss: 0.648289\n",
      "epoch 45; iter: 0; batch classifier loss: 0.420007; batch adversarial loss: 0.549490\n",
      "epoch 45; iter: 200; batch classifier loss: 0.465785; batch adversarial loss: 0.645715\n",
      "epoch 46; iter: 0; batch classifier loss: 0.438219; batch adversarial loss: 0.628294\n",
      "epoch 46; iter: 200; batch classifier loss: 0.393744; batch adversarial loss: 0.556007\n",
      "epoch 47; iter: 0; batch classifier loss: 0.419823; batch adversarial loss: 0.587729\n",
      "epoch 47; iter: 200; batch classifier loss: 0.396425; batch adversarial loss: 0.543087\n",
      "epoch 48; iter: 0; batch classifier loss: 0.457611; batch adversarial loss: 0.619549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48; iter: 200; batch classifier loss: 0.413488; batch adversarial loss: 0.552195\n",
      "epoch 49; iter: 0; batch classifier loss: 0.418760; batch adversarial loss: 0.593718\n",
      "epoch 49; iter: 200; batch classifier loss: 0.446568; batch adversarial loss: 0.588043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x1399bcfd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'debiased_classifier/Softmax:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.217876\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.221187\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - dataset metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.090157\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.094732\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.804955\n",
      "Test set: Balanced classification accuracy = 0.666400\n",
      "Test set: Disparate impact = 0.000000\n",
      "Test set: Equal opportunity difference = -0.470687\n",
      "Test set: Average odds difference = -0.291055\n",
      "Test set: Theil_index = 0.175113\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.792056\n",
      "Test set: Balanced classification accuracy = 0.672481\n",
      "Test set: Disparate impact = 0.553746\n",
      "Test set: Equal opportunity difference = -0.090716\n",
      "Test set: Average odds difference = -0.053841\n",
      "Test set: Theil_index = 0.170358\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    References:\n",
    "    [1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "    AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
